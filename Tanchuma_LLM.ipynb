{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNns5MyWqbQea7K5ivTbxE6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anutkk/llms/blob/master/Tanchuma_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tanchuma - char-level tokenization"
      ],
      "metadata": {
        "id": "oFXFaOchu8yq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Rg2gyBxnuGmt"
      },
      "outputs": [],
      "source": [
        "# !pip install --progress-bar off torch numpy transformers datasets tiktoken wandb tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/karpathy/nanoGPT.git\n",
        "# !cp nanoGPT/model.py ."
      ],
      "metadata": {
        "id": "MgMzUXxgvTZq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# try:\n",
        "#   import google.colab\n",
        "#   IN_COLAB = True\n",
        "# except:\n",
        "#   IN_COLAB = False\n",
        "# #for colab only\n",
        "# if IN_COLAB:\n",
        "#   !git clone https://github.com/anutkk/llms.git\n",
        "#   !cp llms/tanchuma_text.py .\n",
        "#   !cp -r llms/tanchuma_text ."
      ],
      "metadata": {
        "id": "8FeZzllM0AxQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download and tokenize data:"
      ],
      "metadata": {
        "id": "3dVMVra0wd30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Prepare the Shakespeare dataset for character-level language modeling.\n",
        "So instead of encoding with GPT-2 BPE tokens, we just map characters to ints.\n",
        "Will save train.bin, val.bin containing the ids, and meta.pkl containing the\n",
        "encoder and decoder and some other related info.\n",
        "\"\"\"\n",
        "import os\n",
        "import pickle\n",
        "import requests\n",
        "import numpy as np\n",
        "import tanchuma_text\n",
        "\n",
        "train_data, test_data = tanchuma_text.get_data()\n",
        "\n",
        "data = \"\\n\".join(train_data) + \"\\n\".join(test_data)\n",
        "\n",
        "print(f\"length of dataset in characters: {len(data):,}\")\n",
        "\n",
        "# get all the unique characters that occur in this text\n",
        "chars = sorted(list(set(data)))\n",
        "vocab_size = len(chars)\n",
        "print(\"all the unique characters:\", ''.join(chars))\n",
        "print(f\"vocab size: {vocab_size:,}\")\n",
        "\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "def encode(s):\n",
        "    return [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "def decode(l):\n",
        "    return ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# create the train and test splits\n",
        "n = len(data)\n",
        "train_data = data[:int(n*0.9)]\n",
        "val_data = data[int(n*0.9):]\n",
        "\n",
        "# encode both to integers\n",
        "train_ids = encode(train_data)\n",
        "val_ids = encode(val_data)\n",
        "print(f\"train has {len(train_ids):,} tokens\")\n",
        "print(f\"val has {len(val_ids):,} tokens\")\n",
        "\n",
        "# export to bin files\n",
        "train_ids = np.array(train_ids, dtype=np.uint16)\n",
        "val_ids = np.array(val_ids, dtype=np.uint16)\n",
        "train_ids.tofile('train.bin')\n",
        "val_ids.tofile('val.bin')\n",
        "\n",
        "# save the meta information as well, to help us encode/decode later\n",
        "meta = {\n",
        "    'vocab_size': vocab_size,\n",
        "    'itos': itos,\n",
        "    'stoi': stoi,\n",
        "}\n",
        "with open('meta.pkl', 'wb') as f:\n",
        "    pickle.dump(meta, f)\n",
        "\n",
        "# length of dataset in characters: 3,156,907\n",
        "# all the unique characters:\n",
        "#  \"'אבגדהוזחטיךכלםמןנסעףפץצקרשת\n",
        "# vocab size: 31\n",
        "# train has 2,841,216 tokens\n",
        "# val has 315,691 tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TclHqxeYv9Yl",
        "outputId": "0365022a-56c5-40e4-81e4-771375d4a26c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 3,156,907\n",
            "all the unique characters: \n",
            " \"'אבגדהוזחטיךכלםמןנסעףפץצקרשת\n",
            "vocab size: 31\n",
            "train has 2,841,216 tokens\n",
            "val has 315,691 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define model architecture:"
      ],
      "metadata": {
        "id": "LWWyC_6OCvoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train a miniature character-level shakespeare model\n",
        "# good for debugging and playing on macbooks and such\n",
        "\n",
        "out_dir = 'out-shakespeare-char'\n",
        "eval_interval = 250 # keep frequent because we'll overfit\n",
        "eval_iters = 200\n",
        "log_interval = 10 # don't print too too often\n",
        "\n",
        "# we expect to overfit on this small dataset, so only save when val improves\n",
        "always_save_checkpoint = False\n",
        "\n",
        "\n",
        "dataset = 'shakespeare_char'\n",
        "gradient_accumulation_steps = 1\n",
        "batch_size = 64\n",
        "block_size = 256 # context of up to 256 previous characters\n",
        "\n",
        "# baby GPT model :)\n",
        "n_layer = 10\n",
        "n_head = 8\n",
        "n_embd = 384*2\n",
        "dropout = 0.2\n",
        "\n",
        "grad_clip = 1.0\n",
        "\n",
        "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
        "max_iters = 6000\n",
        "lr_decay_iters = 6000 # make equal to max_iters usually\n",
        "min_lr = 1e-4 # learning_rate / 10 usually\n",
        "beta1 = 0.9\n",
        "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
        "\n",
        "weight_decay = 1e-1\n",
        "warmup_iters = 100 # not super necessary potentially"
      ],
      "metadata": {
        "id": "e-lHw2wVwgMb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train:"
      ],
      "metadata": {
        "id": "5SEX-dVPxiKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "import pickle\n",
        "from contextlib import nullcontext\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "\n",
        "from model import GPTConfig, GPT\n",
        "\n",
        "# DDP settings\n",
        "backend = 'nccl' # 'nccl', 'gloo', etc.\n",
        "# system\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "compile = True # use PyTorch 2.0 to compile the model to be faster\n",
        "# -----------------------------------------------------------------------------\n",
        "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
        "config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# various inits, derived attributes, I/O setup\n",
        "master_process = True\n",
        "seed_offset = 0\n",
        "ddp_world_size = 1\n",
        "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "\n",
        "if master_process:\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "torch.manual_seed(1337 + seed_offset)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "# note: float16 data type will automatically use a GradScaler\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "# poor man's data loader\n",
        "data_dir = os.path.join('data', dataset)\n",
        "train_data = np.memmap('train.bin', dtype=np.uint16, mode='r')\n",
        "val_data = np.memmap('val.bin', dtype=np.uint16, mode='r')\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if device_type == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
        "iter_num = 0\n",
        "best_val_loss = 1e9\n",
        "\n",
        "# attempt to derive vocab_size from the dataset\n",
        "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
        "meta_vocab_size = None\n",
        "if os.path.exists(meta_path):\n",
        "    with open(meta_path, 'rb') as f:\n",
        "        meta = pickle.load(f)\n",
        "    meta_vocab_size = meta['vocab_size']\n",
        "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
        "\n",
        "# model init\n",
        "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
        "                  bias=False, vocab_size=None, dropout=dropout) # start with model_args from command line\n",
        "\n",
        "# init a new model from scratch\n",
        "print(\"Initializing a new model from scratch\")\n",
        "# determine the vocab size we'll use for from-scratch training\n",
        "if meta_vocab_size is None:\n",
        "    print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
        "model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
        "gptconf = GPTConfig(**model_args)\n",
        "model = GPT(gptconf)\n",
        "\n",
        "# crop down the model block size if desired, using model surgery\n",
        "if block_size < model.config.block_size:\n",
        "    model.crop_block_size(block_size)\n",
        "    model_args['block_size'] = block_size # so that the checkpoint will have the right value\n",
        "model.to(device)\n",
        "\n",
        "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "\n",
        "# optimizer\n",
        "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
        "checkpoint = None # free up memory\n",
        "\n",
        "# compile the model\n",
        "if compile:\n",
        "    print(\"compiling the model... (takes a ~minute)\")\n",
        "    unoptimized_model = model\n",
        "    model = torch.compile(model) # requires PyTorch 2.0\n",
        "\n",
        "\n",
        "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# learning rate decay scheduler (cosine with warmup)\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)\n",
        "\n",
        "\n",
        "# training loop\n",
        "X, Y = get_batch('train') # fetch the very first batch\n",
        "t0 = time.time()\n",
        "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
        "raw_model = model # unwrap DDP container if needed\n",
        "running_mfu = -1.0\n",
        "while True:\n",
        "\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(iter_num)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # evaluate the loss on train/val sets and write checkpoints\n",
        "    if iter_num % eval_interval == 0 and master_process:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
        "            best_val_loss = losses['val']\n",
        "            if iter_num > 0:\n",
        "                checkpoint = {\n",
        "                    'model': raw_model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'model_args': model_args,\n",
        "                    'iter_num': iter_num,\n",
        "                    'best_val_loss': best_val_loss,\n",
        "                    'config': config,\n",
        "                }\n",
        "                print(f\"saving checkpoint to {out_dir}\")\n",
        "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "\n",
        "\n",
        "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "    # and using the GradScaler if data type is float16\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
        "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
        "        X, Y = get_batch('train')\n",
        "        # backward pass, with gradient scaling if training in fp16\n",
        "        scaler.scale(loss).backward()\n",
        "    # clip the gradient\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    # step the optimizer and scaler if training in fp16\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    # flush the gradients as soon as we can, no need for this memory anymore\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0 and master_process:\n",
        "        # get loss as float. note: this is a CPU-GPU sync point\n",
        "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
        "        lossf = loss.item() * gradient_accumulation_steps\n",
        "        if local_iter_num >= 5: # let the training loop settle a bit\n",
        "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
        "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
        "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
        "    iter_num += 1\n",
        "    local_iter_num += 1\n",
        "\n",
        "    # termination conditions\n",
        "    if iter_num > max_iters:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1N9Ibc-xhYe",
        "outputId": "9d27f9de-fa93-4d79-f82e-fa6052ff98b3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens per iteration will be: 16,384\n",
            "Initializing a new model from scratch\n",
            "defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\n",
            "number of parameters: 109.43M\n",
            "num decayed parameter tensors: 42, with 109,608,960 parameters\n",
            "num non-decayed parameter tensors: 21, with 16,128 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "step 0: train loss 10.8580, val loss 10.8614\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[2023-09-17 21:23:40,484] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-17 21:23:40,804] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-17 21:23:41,488] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-17 21:23:41,783] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-17 21:23:42,237] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-17 21:23:42,533] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-17 21:23:42,978] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-17 21:23:43,271] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-17 21:23:43,712] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-17 21:23:44,014] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-17 21:23:44,457] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-17 21:23:44,751] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-17 21:23:45,206] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-17 21:23:45,517] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-17 21:23:45,964] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-17 21:23:46,256] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-17 21:23:46,691] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-17 21:23:46,992] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-17 21:23:47,438] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-17 21:23:47,997] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 0: loss 10.8786, time 29749.26ms, mfu -100.00%\n",
            "iter 10: loss 6.4341, time 104.46ms, mfu 34.19%\n",
            "iter 20: loss 4.8352, time 105.40ms, mfu 34.16%\n",
            "iter 30: loss 2.9700, time 104.89ms, mfu 34.15%\n",
            "iter 40: loss 2.6100, time 104.47ms, mfu 34.16%\n",
            "iter 50: loss 2.5579, time 104.24ms, mfu 34.17%\n",
            "iter 60: loss 2.5304, time 104.42ms, mfu 34.17%\n",
            "iter 70: loss 2.5314, time 104.89ms, mfu 34.16%\n",
            "iter 80: loss 2.5223, time 104.57ms, mfu 34.16%\n",
            "iter 90: loss 2.5048, time 104.46ms, mfu 34.16%\n",
            "iter 100: loss 2.5229, time 105.03ms, mfu 34.15%\n",
            "iter 110: loss 2.4891, time 105.14ms, mfu 34.13%\n",
            "iter 120: loss 2.4589, time 104.95ms, mfu 34.12%\n",
            "iter 130: loss 2.4594, time 104.69ms, mfu 34.12%\n",
            "iter 140: loss 2.4318, time 105.32ms, mfu 34.10%\n",
            "iter 150: loss 2.4132, time 105.42ms, mfu 34.08%\n",
            "iter 160: loss 2.3339, time 105.26ms, mfu 34.06%\n",
            "iter 170: loss 2.3019, time 104.87ms, mfu 34.06%\n",
            "iter 180: loss 2.2263, time 104.73ms, mfu 34.07%\n",
            "iter 190: loss 2.2196, time 104.70ms, mfu 34.07%\n",
            "iter 200: loss 2.1972, time 104.57ms, mfu 34.08%\n",
            "iter 210: loss 2.1743, time 104.89ms, mfu 34.08%\n",
            "iter 220: loss 2.1345, time 105.10ms, mfu 34.07%\n",
            "iter 230: loss 2.1242, time 105.56ms, mfu 34.04%\n",
            "iter 240: loss 2.1028, time 104.75ms, mfu 34.05%\n",
            "step 250: train loss 2.0358, val loss 2.0438\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 250: loss 2.0523, time 21084.93ms, mfu 30.66%\n",
            "iter 260: loss 2.0557, time 105.16ms, mfu 30.99%\n",
            "iter 270: loss 2.0705, time 104.86ms, mfu 31.30%\n",
            "iter 280: loss 2.0354, time 104.99ms, mfu 31.57%\n",
            "iter 290: loss 2.0443, time 105.51ms, mfu 31.80%\n",
            "iter 300: loss 2.0508, time 105.72ms, mfu 32.00%\n",
            "iter 310: loss 2.0111, time 104.84ms, mfu 32.20%\n",
            "iter 320: loss 2.0154, time 105.05ms, mfu 32.38%\n",
            "iter 330: loss 2.0102, time 105.12ms, mfu 32.54%\n",
            "iter 340: loss 1.9820, time 105.39ms, mfu 32.68%\n",
            "iter 350: loss 1.9928, time 105.15ms, mfu 32.81%\n",
            "iter 360: loss 1.9832, time 104.90ms, mfu 32.93%\n",
            "iter 370: loss 1.9551, time 105.33ms, mfu 33.03%\n",
            "iter 380: loss 1.9677, time 105.38ms, mfu 33.12%\n",
            "iter 390: loss 1.9374, time 104.65ms, mfu 33.22%\n",
            "iter 400: loss 1.9717, time 105.02ms, mfu 33.30%\n",
            "iter 410: loss 1.9460, time 105.50ms, mfu 33.35%\n",
            "iter 420: loss 1.8992, time 105.32ms, mfu 33.41%\n",
            "iter 430: loss 1.9441, time 105.52ms, mfu 33.45%\n",
            "iter 440: loss 1.9314, time 105.44ms, mfu 33.49%\n",
            "iter 450: loss 1.9455, time 105.29ms, mfu 33.54%\n",
            "iter 460: loss 1.9278, time 105.66ms, mfu 33.56%\n",
            "iter 470: loss 1.9042, time 105.56ms, mfu 33.59%\n",
            "iter 480: loss 1.8824, time 105.48ms, mfu 33.62%\n",
            "iter 490: loss 1.9093, time 105.73ms, mfu 33.63%\n",
            "step 500: train loss 1.8118, val loss 1.8267\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 500: loss 1.9176, time 17013.09ms, mfu 30.29%\n",
            "iter 510: loss 1.8500, time 105.33ms, mfu 30.65%\n",
            "iter 520: loss 1.8834, time 105.29ms, mfu 30.98%\n",
            "iter 530: loss 1.8545, time 106.01ms, mfu 31.25%\n",
            "iter 540: loss 1.8574, time 106.22ms, mfu 31.49%\n",
            "iter 550: loss 1.8417, time 105.81ms, mfu 31.72%\n",
            "iter 560: loss 1.8174, time 105.85ms, mfu 31.92%\n",
            "iter 570: loss 1.8470, time 104.56ms, mfu 32.14%\n",
            "iter 580: loss 1.8648, time 104.78ms, mfu 32.34%\n",
            "iter 590: loss 1.8198, time 105.05ms, mfu 32.50%\n",
            "iter 600: loss 1.8184, time 105.83ms, mfu 32.63%\n",
            "iter 610: loss 1.8693, time 105.65ms, mfu 32.75%\n",
            "iter 620: loss 1.8471, time 105.63ms, mfu 32.85%\n",
            "iter 630: loss 1.8257, time 105.03ms, mfu 32.97%\n",
            "iter 640: loss 1.8238, time 105.22ms, mfu 33.07%\n",
            "iter 650: loss 1.8202, time 105.82ms, mfu 33.13%\n",
            "iter 660: loss 1.7952, time 104.79ms, mfu 33.23%\n",
            "iter 670: loss 1.8009, time 105.46ms, mfu 33.29%\n",
            "iter 680: loss 1.8138, time 105.17ms, mfu 33.36%\n",
            "iter 690: loss 1.7931, time 105.03ms, mfu 33.42%\n",
            "iter 700: loss 1.8123, time 106.24ms, mfu 33.44%\n",
            "iter 710: loss 1.7786, time 106.18ms, mfu 33.46%\n",
            "iter 720: loss 1.7483, time 105.21ms, mfu 33.51%\n",
            "iter 730: loss 1.7831, time 104.96ms, mfu 33.56%\n",
            "iter 740: loss 1.7802, time 105.74ms, mfu 33.59%\n",
            "step 750: train loss 1.7066, val loss 1.7281\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 750: loss 1.7642, time 16966.73ms, mfu 30.25%\n",
            "iter 760: loss 1.8016, time 105.79ms, mfu 30.60%\n",
            "iter 770: loss 1.7482, time 105.55ms, mfu 30.92%\n",
            "iter 780: loss 1.7593, time 105.06ms, mfu 31.23%\n",
            "iter 790: loss 1.7675, time 106.53ms, mfu 31.46%\n",
            "iter 800: loss 1.7609, time 105.11ms, mfu 31.71%\n",
            "iter 810: loss 1.7255, time 105.56ms, mfu 31.92%\n",
            "iter 820: loss 1.7575, time 105.62ms, mfu 32.11%\n",
            "iter 830: loss 1.7401, time 106.05ms, mfu 32.27%\n",
            "iter 840: loss 1.7437, time 105.11ms, mfu 32.44%\n",
            "iter 850: loss 1.7194, time 105.36ms, mfu 32.59%\n",
            "iter 860: loss 1.7329, time 105.41ms, mfu 32.72%\n",
            "iter 870: loss 1.7423, time 105.19ms, mfu 32.84%\n",
            "iter 880: loss 1.7121, time 105.32ms, mfu 32.95%\n",
            "iter 890: loss 1.7456, time 104.98ms, mfu 33.06%\n",
            "iter 900: loss 1.7681, time 104.85ms, mfu 33.16%\n",
            "iter 910: loss 1.7000, time 105.12ms, mfu 33.24%\n",
            "iter 920: loss 1.7157, time 105.54ms, mfu 33.30%\n",
            "iter 930: loss 1.7189, time 105.65ms, mfu 33.35%\n",
            "iter 940: loss 1.7172, time 105.35ms, mfu 33.41%\n",
            "iter 950: loss 1.6875, time 105.33ms, mfu 33.46%\n",
            "iter 960: loss 1.7127, time 105.15ms, mfu 33.51%\n",
            "iter 970: loss 1.6893, time 105.25ms, mfu 33.55%\n",
            "iter 980: loss 1.6781, time 105.58ms, mfu 33.58%\n",
            "iter 990: loss 1.7027, time 105.36ms, mfu 33.61%\n",
            "step 1000: train loss 1.6173, val loss 1.6448\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1000: loss 1.7034, time 16899.73ms, mfu 30.27%\n",
            "iter 1010: loss 1.7333, time 105.26ms, mfu 30.64%\n",
            "iter 1020: loss 1.6828, time 105.30ms, mfu 30.96%\n",
            "iter 1030: loss 1.6654, time 105.43ms, mfu 31.26%\n",
            "iter 1040: loss 1.6673, time 105.46ms, mfu 31.52%\n",
            "iter 1050: loss 1.6499, time 105.46ms, mfu 31.75%\n",
            "iter 1060: loss 1.6636, time 105.20ms, mfu 31.97%\n",
            "iter 1070: loss 1.6700, time 104.85ms, mfu 32.18%\n",
            "iter 1080: loss 1.6722, time 105.57ms, mfu 32.35%\n",
            "iter 1090: loss 1.6627, time 105.66ms, mfu 32.49%\n",
            "iter 1100: loss 1.6886, time 105.36ms, mfu 32.63%\n",
            "iter 1110: loss 1.6698, time 105.16ms, mfu 32.77%\n",
            "iter 1120: loss 1.6362, time 105.45ms, mfu 32.88%\n",
            "iter 1130: loss 1.6778, time 104.92ms, mfu 32.99%\n",
            "iter 1140: loss 1.6585, time 105.20ms, mfu 33.09%\n",
            "iter 1150: loss 1.6706, time 105.09ms, mfu 33.18%\n",
            "iter 1160: loss 1.6473, time 104.86ms, mfu 33.27%\n",
            "iter 1170: loss 1.6571, time 105.20ms, mfu 33.34%\n",
            "iter 1180: loss 1.6304, time 106.40ms, mfu 33.36%\n",
            "iter 1190: loss 1.6420, time 104.85ms, mfu 33.43%\n",
            "iter 1200: loss 1.6634, time 105.69ms, mfu 33.47%\n",
            "iter 1210: loss 1.6294, time 105.83ms, mfu 33.49%\n",
            "iter 1220: loss 1.6996, time 105.59ms, mfu 33.53%\n",
            "iter 1230: loss 1.6554, time 105.08ms, mfu 33.57%\n",
            "iter 1240: loss 1.6056, time 105.48ms, mfu 33.60%\n",
            "step 1250: train loss 1.5565, val loss 1.5892\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1250: loss 1.6451, time 16929.65ms, mfu 30.26%\n",
            "iter 1260: loss 1.6212, time 104.98ms, mfu 30.64%\n",
            "iter 1270: loss 1.6120, time 104.90ms, mfu 30.98%\n",
            "iter 1280: loss 1.5980, time 104.93ms, mfu 31.29%\n",
            "iter 1290: loss 1.5780, time 105.27ms, mfu 31.55%\n",
            "iter 1300: loss 1.5841, time 105.05ms, mfu 31.80%\n",
            "iter 1310: loss 1.5822, time 105.73ms, mfu 31.99%\n",
            "iter 1320: loss 1.6328, time 105.57ms, mfu 32.18%\n",
            "iter 1330: loss 1.6483, time 105.66ms, mfu 32.34%\n",
            "iter 1340: loss 1.6015, time 105.15ms, mfu 32.50%\n",
            "iter 1350: loss 1.5996, time 104.89ms, mfu 32.66%\n",
            "iter 1360: loss 1.6060, time 105.30ms, mfu 32.78%\n",
            "iter 1370: loss 1.5737, time 105.59ms, mfu 32.89%\n",
            "iter 1380: loss 1.6072, time 104.96ms, mfu 33.00%\n",
            "iter 1390: loss 1.5991, time 105.14ms, mfu 33.10%\n",
            "iter 1400: loss 1.5843, time 105.07ms, mfu 33.19%\n",
            "iter 1410: loss 1.5949, time 105.18ms, mfu 33.27%\n",
            "iter 1420: loss 1.5823, time 106.04ms, mfu 33.31%\n",
            "iter 1430: loss 1.5606, time 105.58ms, mfu 33.36%\n",
            "iter 1440: loss 1.5770, time 105.92ms, mfu 33.40%\n",
            "iter 1450: loss 1.5650, time 105.50ms, mfu 33.44%\n",
            "iter 1460: loss 1.5820, time 105.48ms, mfu 33.48%\n",
            "iter 1470: loss 1.5674, time 104.54ms, mfu 33.55%\n",
            "iter 1480: loss 1.5909, time 105.29ms, mfu 33.59%\n",
            "iter 1490: loss 1.5820, time 105.40ms, mfu 33.62%\n",
            "step 1500: train loss 1.4918, val loss 1.5439\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1500: loss 1.5743, time 16924.15ms, mfu 30.28%\n",
            "iter 1510: loss 1.5698, time 105.15ms, mfu 30.65%\n",
            "iter 1520: loss 1.5390, time 105.18ms, mfu 30.98%\n",
            "iter 1530: loss 1.5666, time 105.93ms, mfu 31.25%\n",
            "iter 1540: loss 1.5288, time 105.81ms, mfu 31.50%\n",
            "iter 1550: loss 1.5451, time 105.53ms, mfu 31.74%\n",
            "iter 1560: loss 1.5534, time 104.93ms, mfu 31.97%\n",
            "iter 1570: loss 1.5726, time 105.36ms, mfu 32.16%\n",
            "iter 1580: loss 1.5593, time 105.05ms, mfu 32.34%\n",
            "iter 1590: loss 1.5887, time 104.93ms, mfu 32.51%\n",
            "iter 1600: loss 1.5455, time 105.33ms, mfu 32.65%\n",
            "iter 1610: loss 1.5507, time 104.78ms, mfu 32.80%\n",
            "iter 1620: loss 1.5417, time 104.92ms, mfu 32.92%\n",
            "iter 1630: loss 1.5318, time 104.93ms, mfu 33.03%\n",
            "iter 1640: loss 1.5667, time 105.57ms, mfu 33.11%\n",
            "iter 1650: loss 1.5619, time 105.81ms, mfu 33.18%\n",
            "iter 1660: loss 1.5035, time 105.83ms, mfu 33.23%\n",
            "iter 1670: loss 1.5816, time 105.49ms, mfu 33.30%\n",
            "iter 1680: loss 1.5209, time 105.10ms, mfu 33.37%\n",
            "iter 1690: loss 1.5463, time 105.29ms, mfu 33.42%\n",
            "iter 1700: loss 1.5332, time 105.55ms, mfu 33.46%\n",
            "iter 1710: loss 1.5103, time 105.09ms, mfu 33.52%\n",
            "iter 1720: loss 1.5155, time 105.23ms, mfu 33.56%\n",
            "iter 1730: loss 1.4860, time 104.90ms, mfu 33.61%\n",
            "iter 1740: loss 1.5440, time 105.70ms, mfu 33.63%\n",
            "step 1750: train loss 1.4420, val loss 1.5092\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1750: loss 1.5028, time 17031.56ms, mfu 30.28%\n",
            "iter 1760: loss 1.4917, time 104.45ms, mfu 30.68%\n",
            "iter 1770: loss 1.5268, time 105.14ms, mfu 31.01%\n",
            "iter 1780: loss 1.5277, time 105.45ms, mfu 31.29%\n",
            "iter 1790: loss 1.4980, time 105.18ms, mfu 31.56%\n",
            "iter 1800: loss 1.4835, time 105.17ms, mfu 31.80%\n",
            "iter 1810: loss 1.4992, time 105.20ms, mfu 32.01%\n",
            "iter 1820: loss 1.4894, time 105.32ms, mfu 32.20%\n",
            "iter 1830: loss 1.5046, time 105.65ms, mfu 32.36%\n",
            "iter 1840: loss 1.4896, time 106.16ms, mfu 32.49%\n",
            "iter 1850: loss 1.5125, time 105.07ms, mfu 32.64%\n",
            "iter 1860: loss 1.4862, time 105.03ms, mfu 32.78%\n",
            "iter 1870: loss 1.4496, time 105.44ms, mfu 32.89%\n",
            "iter 1880: loss 1.5268, time 104.93ms, mfu 33.00%\n",
            "iter 1890: loss 1.4925, time 105.44ms, mfu 33.09%\n",
            "iter 1900: loss 1.5236, time 105.20ms, mfu 33.18%\n",
            "iter 1910: loss 1.4895, time 105.12ms, mfu 33.26%\n",
            "iter 1920: loss 1.4701, time 105.45ms, mfu 33.32%\n",
            "iter 1930: loss 1.4664, time 105.59ms, mfu 33.37%\n",
            "iter 1940: loss 1.4697, time 105.42ms, mfu 33.42%\n",
            "iter 1950: loss 1.4958, time 105.68ms, mfu 33.46%\n",
            "iter 1960: loss 1.4631, time 105.11ms, mfu 33.51%\n",
            "iter 1970: loss 1.4717, time 105.32ms, mfu 33.55%\n",
            "iter 1980: loss 1.5016, time 105.07ms, mfu 33.59%\n",
            "iter 1990: loss 1.4727, time 105.10ms, mfu 33.63%\n",
            "step 2000: train loss 1.3901, val loss 1.4762\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2000: loss 1.4615, time 21271.17ms, mfu 30.29%\n",
            "iter 2010: loss 1.4976, time 104.99ms, mfu 30.66%\n",
            "iter 2020: loss 1.4564, time 104.66ms, mfu 31.01%\n",
            "iter 2030: loss 1.4805, time 105.03ms, mfu 31.31%\n",
            "iter 2040: loss 1.4664, time 105.28ms, mfu 31.57%\n",
            "iter 2050: loss 1.4939, time 105.34ms, mfu 31.80%\n",
            "iter 2060: loss 1.4580, time 105.92ms, mfu 31.99%\n",
            "iter 2070: loss 1.4610, time 105.87ms, mfu 32.17%\n",
            "iter 2080: loss 1.4716, time 105.15ms, mfu 32.35%\n",
            "iter 2090: loss 1.4830, time 106.06ms, mfu 32.48%\n",
            "iter 2100: loss 1.4448, time 105.33ms, mfu 32.62%\n",
            "iter 2110: loss 1.4469, time 105.22ms, mfu 32.76%\n",
            "iter 2120: loss 1.4378, time 105.23ms, mfu 32.88%\n",
            "iter 2130: loss 1.4575, time 105.29ms, mfu 32.98%\n",
            "iter 2140: loss 1.4277, time 105.06ms, mfu 33.08%\n",
            "iter 2150: loss 1.5178, time 104.97ms, mfu 33.18%\n",
            "iter 2160: loss 1.4386, time 105.97ms, mfu 33.23%\n",
            "iter 2170: loss 1.3808, time 105.58ms, mfu 33.29%\n",
            "iter 2180: loss 1.4326, time 105.10ms, mfu 33.36%\n",
            "iter 2190: loss 1.4497, time 105.53ms, mfu 33.41%\n",
            "iter 2200: loss 1.4200, time 105.09ms, mfu 33.47%\n",
            "iter 2210: loss 1.4384, time 105.02ms, mfu 33.52%\n",
            "iter 2220: loss 1.3883, time 104.96ms, mfu 33.57%\n",
            "iter 2230: loss 1.4126, time 106.02ms, mfu 33.58%\n",
            "iter 2240: loss 1.4264, time 105.39ms, mfu 33.61%\n",
            "step 2250: train loss 1.3407, val loss 1.4482\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2250: loss 1.4441, time 16893.75ms, mfu 30.27%\n",
            "iter 2260: loss 1.4393, time 105.14ms, mfu 30.64%\n",
            "iter 2270: loss 1.4022, time 105.16ms, mfu 30.98%\n",
            "iter 2280: loss 1.4289, time 105.44ms, mfu 31.27%\n",
            "iter 2290: loss 1.4091, time 106.01ms, mfu 31.51%\n",
            "iter 2300: loss 1.4297, time 105.76ms, mfu 31.73%\n",
            "iter 2310: loss 1.4216, time 106.39ms, mfu 31.92%\n",
            "iter 2320: loss 1.4028, time 104.75ms, mfu 32.14%\n",
            "iter 2330: loss 1.4132, time 105.97ms, mfu 32.29%\n",
            "iter 2340: loss 1.4487, time 105.63ms, mfu 32.44%\n",
            "iter 2350: loss 1.3482, time 105.37ms, mfu 32.59%\n",
            "iter 2360: loss 1.4108, time 105.41ms, mfu 32.72%\n",
            "iter 2370: loss 1.3888, time 105.70ms, mfu 32.83%\n",
            "iter 2380: loss 1.4083, time 105.32ms, mfu 32.94%\n",
            "iter 2390: loss 1.4109, time 105.78ms, mfu 33.02%\n",
            "iter 2400: loss 1.4237, time 105.09ms, mfu 33.12%\n",
            "iter 2410: loss 1.3916, time 105.39ms, mfu 33.19%\n",
            "iter 2420: loss 1.3754, time 104.82ms, mfu 33.28%\n",
            "iter 2430: loss 1.3993, time 105.20ms, mfu 33.35%\n",
            "iter 2440: loss 1.4063, time 105.35ms, mfu 33.40%\n",
            "iter 2450: loss 1.3907, time 105.13ms, mfu 33.46%\n",
            "iter 2460: loss 1.3695, time 105.66ms, mfu 33.49%\n",
            "iter 2470: loss 1.3868, time 105.52ms, mfu 33.53%\n",
            "iter 2480: loss 1.3851, time 105.94ms, mfu 33.55%\n",
            "iter 2490: loss 1.3975, time 106.05ms, mfu 33.56%\n",
            "step 2500: train loss 1.2852, val loss 1.4276\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2500: loss 1.3681, time 18842.38ms, mfu 30.22%\n",
            "iter 2510: loss 1.3876, time 105.45ms, mfu 30.59%\n",
            "iter 2520: loss 1.3768, time 105.42ms, mfu 30.92%\n",
            "iter 2530: loss 1.3575, time 105.34ms, mfu 31.22%\n",
            "iter 2540: loss 1.3294, time 105.27ms, mfu 31.49%\n",
            "iter 2550: loss 1.3385, time 104.96ms, mfu 31.74%\n",
            "iter 2560: loss 1.4017, time 105.10ms, mfu 31.97%\n",
            "iter 2570: loss 1.3493, time 105.03ms, mfu 32.17%\n",
            "iter 2580: loss 1.3622, time 105.01ms, mfu 32.35%\n",
            "iter 2590: loss 1.3236, time 104.76ms, mfu 32.53%\n",
            "iter 2600: loss 1.3619, time 105.29ms, mfu 32.67%\n",
            "iter 2610: loss 1.3721, time 104.96ms, mfu 32.80%\n",
            "iter 2620: loss 1.3831, time 105.39ms, mfu 32.91%\n",
            "iter 2630: loss 1.3637, time 105.98ms, mfu 32.99%\n",
            "iter 2640: loss 1.3571, time 105.28ms, mfu 33.09%\n",
            "iter 2650: loss 1.3657, time 105.16ms, mfu 33.17%\n",
            "iter 2660: loss 1.3261, time 105.20ms, mfu 33.25%\n",
            "iter 2670: loss 1.3421, time 105.20ms, mfu 33.32%\n",
            "iter 2680: loss 1.3161, time 104.87ms, mfu 33.40%\n",
            "iter 2690: loss 1.3465, time 105.75ms, mfu 33.43%\n",
            "iter 2700: loss 1.3464, time 106.29ms, mfu 33.45%\n",
            "iter 2710: loss 1.3459, time 105.73ms, mfu 33.48%\n",
            "iter 2720: loss 1.3424, time 105.88ms, mfu 33.51%\n",
            "iter 2730: loss 1.3278, time 104.94ms, mfu 33.56%\n",
            "iter 2740: loss 1.2998, time 105.60ms, mfu 33.59%\n",
            "step 2750: train loss 1.2313, val loss 1.3963\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2750: loss 1.3483, time 16879.82ms, mfu 30.25%\n",
            "iter 2760: loss 1.3504, time 105.57ms, mfu 30.61%\n",
            "iter 2770: loss 1.3035, time 105.27ms, mfu 30.94%\n",
            "iter 2780: loss 1.3382, time 105.95ms, mfu 31.22%\n",
            "iter 2790: loss 1.3131, time 105.17ms, mfu 31.49%\n",
            "iter 2800: loss 1.3256, time 106.16ms, mfu 31.71%\n",
            "iter 2810: loss 1.3033, time 105.03ms, mfu 31.94%\n",
            "iter 2820: loss 1.3303, time 105.60ms, mfu 32.13%\n",
            "iter 2830: loss 1.3460, time 106.25ms, mfu 32.27%\n",
            "iter 2840: loss 1.2881, time 105.26ms, mfu 32.44%\n",
            "iter 2850: loss 1.3272, time 105.07ms, mfu 32.60%\n",
            "iter 2860: loss 1.3143, time 105.17ms, mfu 32.73%\n",
            "iter 2870: loss 1.3303, time 105.08ms, mfu 32.86%\n",
            "iter 2880: loss 1.3149, time 105.46ms, mfu 32.96%\n",
            "iter 2890: loss 1.2696, time 104.84ms, mfu 33.07%\n",
            "iter 2900: loss 1.2741, time 124.56ms, mfu 32.63%\n",
            "iter 2910: loss 1.2915, time 105.46ms, mfu 32.75%\n",
            "iter 2920: loss 1.2855, time 105.67ms, mfu 32.86%\n",
            "iter 2930: loss 1.3194, time 106.20ms, mfu 32.94%\n",
            "iter 2940: loss 1.2956, time 105.16ms, mfu 33.04%\n",
            "iter 2950: loss 1.2873, time 105.27ms, mfu 33.13%\n",
            "iter 2960: loss 1.3015, time 104.90ms, mfu 33.22%\n",
            "iter 2970: loss 1.2566, time 105.15ms, mfu 33.29%\n",
            "iter 2980: loss 1.3085, time 105.54ms, mfu 33.35%\n",
            "iter 2990: loss 1.3492, time 104.74ms, mfu 33.42%\n",
            "step 3000: train loss 1.1756, val loss 1.3803\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 3000: loss 1.2862, time 16910.39ms, mfu 30.10%\n",
            "iter 3010: loss 1.3011, time 105.01ms, mfu 30.49%\n",
            "iter 3020: loss 1.2949, time 105.44ms, mfu 30.83%\n",
            "iter 3030: loss 1.2915, time 104.99ms, mfu 31.15%\n",
            "iter 3040: loss 1.2872, time 105.43ms, mfu 31.42%\n",
            "iter 3050: loss 1.2539, time 105.38ms, mfu 31.67%\n",
            "iter 3060: loss 1.2683, time 105.41ms, mfu 31.89%\n",
            "iter 3070: loss 1.2661, time 105.02ms, mfu 32.10%\n",
            "iter 3080: loss 1.2440, time 109.07ms, mfu 32.17%\n",
            "iter 3090: loss 1.2333, time 105.32ms, mfu 32.34%\n",
            "iter 3100: loss 1.2680, time 105.46ms, mfu 32.50%\n",
            "iter 3110: loss 1.2538, time 105.23ms, mfu 32.64%\n",
            "iter 3120: loss 1.2746, time 105.92ms, mfu 32.75%\n",
            "iter 3130: loss 1.2923, time 105.56ms, mfu 32.86%\n",
            "iter 3140: loss 1.2535, time 104.89ms, mfu 32.98%\n",
            "iter 3150: loss 1.2928, time 105.05ms, mfu 33.08%\n",
            "iter 3160: loss 1.2176, time 105.07ms, mfu 33.17%\n",
            "iter 3170: loss 1.2377, time 105.10ms, mfu 33.25%\n",
            "iter 3180: loss 1.2415, time 105.67ms, mfu 33.31%\n",
            "iter 3190: loss 1.2267, time 106.50ms, mfu 33.33%\n",
            "iter 3200: loss 1.2709, time 105.73ms, mfu 33.37%\n",
            "iter 3210: loss 1.2181, time 104.90ms, mfu 33.44%\n",
            "iter 3220: loss 1.2268, time 104.81ms, mfu 33.51%\n",
            "iter 3230: loss 1.2499, time 105.61ms, mfu 33.54%\n",
            "iter 3240: loss 1.2403, time 105.04ms, mfu 33.58%\n",
            "step 3250: train loss 1.1195, val loss 1.3747\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 3250: loss 1.2338, time 16872.87ms, mfu 30.25%\n",
            "iter 3260: loss 1.2325, time 105.31ms, mfu 30.61%\n",
            "iter 3270: loss 1.2522, time 105.26ms, mfu 30.95%\n",
            "iter 3280: loss 1.2066, time 105.14ms, mfu 31.25%\n",
            "iter 3290: loss 1.2267, time 105.07ms, mfu 31.52%\n",
            "iter 3300: loss 1.2351, time 105.66ms, mfu 31.75%\n",
            "iter 3310: loss 1.2305, time 104.89ms, mfu 31.98%\n",
            "iter 3320: loss 1.2682, time 105.36ms, mfu 32.17%\n",
            "iter 3330: loss 1.2455, time 105.31ms, mfu 32.35%\n",
            "iter 3340: loss 1.1649, time 104.88ms, mfu 32.52%\n",
            "iter 3350: loss 1.2303, time 105.59ms, mfu 32.65%\n",
            "iter 3360: loss 1.1786, time 105.05ms, mfu 32.78%\n",
            "iter 3370: loss 1.2506, time 105.40ms, mfu 32.89%\n",
            "iter 3380: loss 1.1695, time 105.27ms, mfu 33.00%\n",
            "iter 3390: loss 1.1844, time 105.37ms, mfu 33.09%\n",
            "iter 3400: loss 1.2325, time 104.99ms, mfu 33.18%\n",
            "iter 3410: loss 1.2135, time 105.60ms, mfu 33.24%\n",
            "iter 3420: loss 1.2128, time 105.61ms, mfu 33.30%\n",
            "iter 3430: loss 1.2031, time 105.17ms, mfu 33.37%\n",
            "iter 3440: loss 1.2056, time 104.92ms, mfu 33.44%\n",
            "iter 3450: loss 1.1781, time 105.29ms, mfu 33.48%\n",
            "iter 3460: loss 1.1962, time 105.26ms, mfu 33.53%\n",
            "iter 3470: loss 1.2332, time 105.29ms, mfu 33.57%\n",
            "iter 3480: loss 1.1910, time 104.97ms, mfu 33.61%\n",
            "iter 3490: loss 1.1453, time 105.08ms, mfu 33.65%\n",
            "step 3500: train loss 1.0635, val loss 1.3635\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 3500: loss 1.2138, time 16854.22ms, mfu 30.31%\n",
            "iter 3510: loss 1.1764, time 105.69ms, mfu 30.66%\n",
            "iter 3520: loss 1.1835, time 105.38ms, mfu 30.98%\n",
            "iter 3530: loss 1.2108, time 105.53ms, mfu 31.27%\n",
            "iter 3540: loss 1.1980, time 105.69ms, mfu 31.52%\n",
            "iter 3550: loss 1.1907, time 104.92ms, mfu 31.77%\n",
            "iter 3560: loss 1.2459, time 105.09ms, mfu 31.99%\n",
            "iter 3570: loss 1.1908, time 105.10ms, mfu 32.19%\n",
            "iter 3580: loss 1.1643, time 105.03ms, mfu 32.37%\n",
            "iter 3590: loss 1.1499, time 105.29ms, mfu 32.53%\n",
            "iter 3600: loss 1.1461, time 105.26ms, mfu 32.67%\n",
            "iter 3610: loss 1.1495, time 105.67ms, mfu 32.78%\n",
            "iter 3620: loss 1.1555, time 105.40ms, mfu 32.89%\n",
            "iter 3630: loss 1.1948, time 104.92ms, mfu 33.01%\n",
            "iter 3640: loss 1.1753, time 105.13ms, mfu 33.10%\n",
            "iter 3650: loss 1.1370, time 105.78ms, mfu 33.17%\n",
            "iter 3660: loss 1.1480, time 105.89ms, mfu 33.23%\n",
            "iter 3670: loss 1.1403, time 105.18ms, mfu 33.30%\n",
            "iter 3680: loss 1.1504, time 104.87ms, mfu 33.38%\n",
            "iter 3690: loss 1.1794, time 105.22ms, mfu 33.43%\n",
            "iter 3700: loss 1.1523, time 105.16ms, mfu 33.49%\n",
            "iter 3710: loss 1.1420, time 105.11ms, mfu 33.54%\n",
            "iter 3720: loss 1.1499, time 105.81ms, mfu 33.56%\n",
            "iter 3730: loss 1.2006, time 104.84ms, mfu 33.61%\n",
            "iter 3740: loss 1.1387, time 105.09ms, mfu 33.65%\n",
            "step 3750: train loss 1.0104, val loss 1.3509\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 3750: loss 1.1596, time 16903.75ms, mfu 30.30%\n",
            "iter 3760: loss 1.1385, time 105.54ms, mfu 30.66%\n",
            "iter 3770: loss 1.1217, time 105.91ms, mfu 30.96%\n",
            "iter 3780: loss 1.1284, time 105.80ms, mfu 31.24%\n",
            "iter 3790: loss 1.1413, time 105.91ms, mfu 31.49%\n",
            "iter 3800: loss 1.1432, time 106.05ms, mfu 31.71%\n",
            "iter 3810: loss 1.0818, time 106.22ms, mfu 31.90%\n",
            "iter 3820: loss 1.1377, time 105.16ms, mfu 32.11%\n",
            "iter 3830: loss 1.1192, time 105.31ms, mfu 32.29%\n",
            "iter 3840: loss 1.0980, time 104.93ms, mfu 32.46%\n",
            "iter 3850: loss 1.1035, time 105.54ms, mfu 32.60%\n",
            "iter 3860: loss 1.1210, time 106.00ms, mfu 32.71%\n",
            "iter 3870: loss 1.1240, time 105.30ms, mfu 32.83%\n",
            "iter 3880: loss 1.1424, time 105.40ms, mfu 32.94%\n",
            "iter 3890: loss 1.0939, time 104.64ms, mfu 33.06%\n",
            "iter 3900: loss 1.1163, time 105.75ms, mfu 33.13%\n",
            "iter 3910: loss 1.1273, time 105.93ms, mfu 33.19%\n",
            "iter 3920: loss 1.0692, time 105.38ms, mfu 33.26%\n",
            "iter 3930: loss 1.1045, time 105.83ms, mfu 33.31%\n",
            "iter 3940: loss 1.1226, time 105.63ms, mfu 33.36%\n",
            "iter 3950: loss 1.0792, time 105.32ms, mfu 33.41%\n",
            "iter 3960: loss 1.0903, time 106.09ms, mfu 33.44%\n",
            "iter 3970: loss 1.1024, time 106.12ms, mfu 33.46%\n",
            "iter 3980: loss 1.1087, time 105.38ms, mfu 33.50%\n",
            "iter 3990: loss 1.1073, time 105.38ms, mfu 33.54%\n",
            "step 4000: train loss 0.9537, val loss 1.3453\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 4000: loss 1.1290, time 16940.93ms, mfu 30.21%\n",
            "iter 4010: loss 1.1016, time 105.63ms, mfu 30.57%\n",
            "iter 4020: loss 1.0521, time 105.56ms, mfu 30.90%\n",
            "iter 4030: loss 1.1182, time 105.10ms, mfu 31.21%\n",
            "iter 4040: loss 1.0789, time 104.82ms, mfu 31.49%\n",
            "iter 4050: loss 1.0937, time 105.94ms, mfu 31.71%\n",
            "iter 4060: loss 1.1061, time 105.62ms, mfu 31.92%\n",
            "iter 4070: loss 1.0997, time 104.99ms, mfu 32.13%\n",
            "iter 4080: loss 1.0712, time 105.02ms, mfu 32.32%\n",
            "iter 4090: loss 1.0460, time 105.35ms, mfu 32.48%\n",
            "iter 4100: loss 1.1141, time 105.32ms, mfu 32.62%\n",
            "iter 4110: loss 1.1089, time 105.04ms, mfu 32.76%\n",
            "iter 4120: loss 1.0604, time 105.11ms, mfu 32.88%\n",
            "iter 4130: loss 1.0532, time 105.05ms, mfu 33.00%\n",
            "iter 4140: loss 1.0679, time 105.38ms, mfu 33.08%\n",
            "iter 4150: loss 1.0826, time 104.96ms, mfu 33.18%\n",
            "iter 4160: loss 1.0594, time 105.49ms, mfu 33.25%\n",
            "iter 4170: loss 1.0309, time 105.88ms, mfu 33.30%\n",
            "iter 4180: loss 1.0330, time 105.12ms, mfu 33.36%\n",
            "iter 4190: loss 1.1027, time 106.39ms, mfu 33.38%\n",
            "iter 4200: loss 1.0851, time 105.03ms, mfu 33.45%\n",
            "iter 4210: loss 1.0877, time 104.92ms, mfu 33.51%\n",
            "iter 4220: loss 1.0891, time 105.16ms, mfu 33.55%\n",
            "iter 4230: loss 1.0522, time 105.42ms, mfu 33.59%\n",
            "iter 4240: loss 1.0495, time 105.54ms, mfu 33.61%\n",
            "step 4250: train loss 0.9011, val loss 1.3454\n",
            "iter 4250: loss 1.0489, time 14702.76ms, mfu 30.27%\n",
            "iter 4260: loss 1.0623, time 105.75ms, mfu 30.62%\n",
            "iter 4270: loss 1.0274, time 106.37ms, mfu 30.92%\n",
            "iter 4280: loss 1.0064, time 105.63ms, mfu 31.21%\n",
            "iter 4290: loss 1.0388, time 105.20ms, mfu 31.48%\n",
            "iter 4300: loss 1.0346, time 105.18ms, mfu 31.73%\n",
            "iter 4310: loss 1.0592, time 105.50ms, mfu 31.94%\n",
            "iter 4320: loss 1.0247, time 105.76ms, mfu 32.13%\n",
            "iter 4330: loss 0.9774, time 105.65ms, mfu 32.29%\n",
            "iter 4340: loss 1.0552, time 104.89ms, mfu 32.47%\n",
            "iter 4350: loss 1.0326, time 105.29ms, mfu 32.62%\n",
            "iter 4360: loss 1.0140, time 105.70ms, mfu 32.73%\n",
            "iter 4370: loss 1.0478, time 105.76ms, mfu 32.84%\n",
            "iter 4380: loss 1.0265, time 105.90ms, mfu 32.93%\n",
            "iter 4390: loss 1.0100, time 105.51ms, mfu 33.02%\n",
            "iter 4400: loss 1.0086, time 105.71ms, mfu 33.10%\n",
            "iter 4410: loss 1.0135, time 106.67ms, mfu 33.13%\n",
            "iter 4420: loss 1.0456, time 106.63ms, mfu 33.17%\n",
            "iter 4430: loss 1.0299, time 105.51ms, mfu 33.24%\n",
            "iter 4440: loss 1.0122, time 105.53ms, mfu 33.30%\n",
            "iter 4450: loss 0.9868, time 105.18ms, mfu 33.37%\n",
            "iter 4460: loss 1.0047, time 105.42ms, mfu 33.42%\n",
            "iter 4470: loss 1.0037, time 105.71ms, mfu 33.45%\n",
            "iter 4480: loss 1.0482, time 105.19ms, mfu 33.50%\n",
            "iter 4490: loss 1.0029, time 105.34ms, mfu 33.54%\n",
            "step 4500: train loss 0.8551, val loss 1.3402\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 4500: loss 0.9803, time 16984.15ms, mfu 30.21%\n",
            "iter 4510: loss 1.0247, time 105.28ms, mfu 30.58%\n",
            "iter 4520: loss 1.0037, time 106.38ms, mfu 30.88%\n",
            "iter 4530: loss 0.9944, time 105.08ms, mfu 31.19%\n",
            "iter 4540: loss 0.9638, time 105.71ms, mfu 31.45%\n",
            "iter 4550: loss 0.9914, time 105.39ms, mfu 31.70%\n",
            "iter 4560: loss 0.9942, time 105.57ms, mfu 31.91%\n",
            "iter 4570: loss 1.0050, time 105.07ms, mfu 32.12%\n",
            "iter 4580: loss 0.9845, time 105.86ms, mfu 32.28%\n",
            "iter 4590: loss 0.9827, time 105.65ms, mfu 32.43%\n",
            "iter 4600: loss 1.0244, time 105.16ms, mfu 32.59%\n",
            "iter 4610: loss 0.9925, time 105.20ms, mfu 32.72%\n",
            "iter 4620: loss 1.0138, time 105.51ms, mfu 32.84%\n",
            "iter 4630: loss 0.9819, time 105.41ms, mfu 32.94%\n",
            "iter 4640: loss 1.0039, time 105.45ms, mfu 33.03%\n",
            "iter 4650: loss 0.9858, time 105.73ms, mfu 33.11%\n",
            "iter 4660: loss 0.9710, time 105.34ms, mfu 33.19%\n",
            "iter 4670: loss 0.9559, time 105.43ms, mfu 33.26%\n",
            "iter 4680: loss 1.0089, time 104.84ms, mfu 33.34%\n",
            "iter 4690: loss 0.9816, time 105.91ms, mfu 33.38%\n",
            "iter 4700: loss 0.9579, time 105.66ms, mfu 33.42%\n",
            "iter 4710: loss 1.0198, time 105.07ms, mfu 33.48%\n",
            "iter 4720: loss 0.9717, time 105.42ms, mfu 33.52%\n",
            "iter 4730: loss 0.9482, time 106.18ms, mfu 33.53%\n",
            "iter 4740: loss 0.9882, time 105.41ms, mfu 33.57%\n",
            "step 4750: train loss 0.8165, val loss 1.3415\n",
            "iter 4750: loss 0.9776, time 14692.64ms, mfu 30.23%\n",
            "iter 4760: loss 0.9498, time 105.43ms, mfu 30.60%\n",
            "iter 4770: loss 1.0078, time 105.81ms, mfu 30.91%\n",
            "iter 4780: loss 0.9784, time 105.70ms, mfu 31.20%\n",
            "iter 4790: loss 0.9865, time 105.73ms, mfu 31.46%\n",
            "iter 4800: loss 0.9445, time 105.94ms, mfu 31.68%\n",
            "iter 4810: loss 0.9801, time 106.05ms, mfu 31.88%\n",
            "iter 4820: loss 0.9546, time 106.09ms, mfu 32.06%\n",
            "iter 4830: loss 0.9799, time 105.57ms, mfu 32.24%\n",
            "iter 4840: loss 0.9440, time 105.36ms, mfu 32.41%\n",
            "iter 4850: loss 0.9466, time 105.86ms, mfu 32.54%\n",
            "iter 4860: loss 0.9861, time 106.69ms, mfu 32.63%\n",
            "iter 4870: loss 0.9624, time 105.72ms, mfu 32.75%\n",
            "iter 4880: loss 0.9534, time 105.02ms, mfu 32.87%\n",
            "iter 4890: loss 0.9448, time 105.63ms, mfu 32.97%\n",
            "iter 4900: loss 0.9841, time 105.92ms, mfu 33.04%\n",
            "iter 4910: loss 0.9687, time 105.07ms, mfu 33.14%\n",
            "iter 4920: loss 0.9602, time 105.46ms, mfu 33.21%\n",
            "iter 4930: loss 0.9104, time 105.18ms, mfu 33.29%\n",
            "iter 4940: loss 0.9561, time 105.28ms, mfu 33.35%\n",
            "iter 4950: loss 0.9306, time 105.13ms, mfu 33.41%\n",
            "iter 4960: loss 0.8874, time 105.41ms, mfu 33.46%\n",
            "iter 4970: loss 0.9809, time 105.60ms, mfu 33.50%\n",
            "iter 4980: loss 0.9531, time 105.61ms, mfu 33.53%\n",
            "iter 4990: loss 0.9587, time 105.45ms, mfu 33.56%\n",
            "step 5000: train loss 0.7819, val loss 1.3421\n",
            "iter 5000: loss 0.9300, time 14766.96ms, mfu 30.23%\n",
            "iter 5010: loss 0.9351, time 106.25ms, mfu 30.57%\n",
            "iter 5020: loss 0.9344, time 106.04ms, mfu 30.88%\n",
            "iter 5030: loss 0.9231, time 106.04ms, mfu 31.16%\n",
            "iter 5040: loss 0.9354, time 106.72ms, mfu 31.39%\n",
            "iter 5050: loss 0.9427, time 104.74ms, mfu 31.66%\n",
            "iter 5060: loss 0.9068, time 105.62ms, mfu 31.88%\n",
            "iter 5070: loss 0.9464, time 106.18ms, mfu 32.05%\n",
            "iter 5080: loss 0.9479, time 105.60ms, mfu 32.23%\n",
            "iter 5090: loss 0.9091, time 105.16ms, mfu 32.40%\n",
            "iter 5100: loss 0.9474, time 105.04ms, mfu 32.56%\n",
            "iter 5110: loss 0.9435, time 105.07ms, mfu 32.71%\n",
            "iter 5120: loss 0.9077, time 106.06ms, mfu 32.80%\n",
            "iter 5130: loss 0.9227, time 105.46ms, mfu 32.91%\n",
            "iter 5140: loss 0.9398, time 105.24ms, mfu 33.01%\n",
            "iter 5150: loss 0.9209, time 105.17ms, mfu 33.11%\n",
            "iter 5160: loss 0.9341, time 105.75ms, mfu 33.17%\n",
            "iter 5170: loss 0.8921, time 105.54ms, mfu 33.24%\n",
            "iter 5180: loss 0.9417, time 104.93ms, mfu 33.32%\n",
            "iter 5190: loss 0.9024, time 105.57ms, mfu 33.37%\n",
            "iter 5200: loss 0.9988, time 105.88ms, mfu 33.41%\n",
            "iter 5210: loss 0.9050, time 106.01ms, mfu 33.44%\n",
            "iter 5220: loss 0.9350, time 105.52ms, mfu 33.48%\n",
            "iter 5230: loss 0.9101, time 105.80ms, mfu 33.51%\n",
            "iter 5240: loss 0.8869, time 105.76ms, mfu 33.53%\n",
            "step 5250: train loss 0.7461, val loss 1.3452\n",
            "iter 5250: loss 0.8840, time 14701.11ms, mfu 30.20%\n",
            "iter 5260: loss 0.9221, time 105.76ms, mfu 30.56%\n",
            "iter 5270: loss 0.9586, time 105.38ms, mfu 30.89%\n",
            "iter 5280: loss 0.8969, time 105.57ms, mfu 31.19%\n",
            "iter 5290: loss 0.9143, time 105.50ms, mfu 31.45%\n",
            "iter 5300: loss 0.9612, time 105.42ms, mfu 31.70%\n",
            "iter 5310: loss 0.9204, time 105.57ms, mfu 31.91%\n",
            "iter 5320: loss 0.9061, time 105.40ms, mfu 32.11%\n",
            "iter 5330: loss 0.9041, time 105.15ms, mfu 32.29%\n",
            "iter 5340: loss 0.8986, time 105.70ms, mfu 32.44%\n",
            "iter 5350: loss 0.8921, time 105.82ms, mfu 32.57%\n",
            "iter 5360: loss 0.9236, time 105.80ms, mfu 32.69%\n",
            "iter 5370: loss 0.9244, time 105.19ms, mfu 32.82%\n",
            "iter 5380: loss 0.9134, time 105.39ms, mfu 32.93%\n",
            "iter 5390: loss 0.8817, time 105.40ms, mfu 33.02%\n",
            "iter 5400: loss 0.8781, time 105.81ms, mfu 33.10%\n",
            "iter 5410: loss 0.8534, time 106.11ms, mfu 33.15%\n",
            "iter 5420: loss 0.8956, time 105.66ms, mfu 33.22%\n",
            "iter 5430: loss 0.8641, time 105.18ms, mfu 33.29%\n",
            "iter 5440: loss 0.9017, time 105.55ms, mfu 33.35%\n",
            "iter 5450: loss 0.8638, time 105.57ms, mfu 33.40%\n",
            "iter 5460: loss 0.9364, time 105.54ms, mfu 33.44%\n",
            "iter 5470: loss 0.9072, time 105.17ms, mfu 33.49%\n",
            "iter 5480: loss 0.9109, time 105.61ms, mfu 33.52%\n",
            "iter 5490: loss 0.8732, time 105.23ms, mfu 33.57%\n",
            "step 5500: train loss 0.7215, val loss 1.3438\n",
            "iter 5500: loss 0.8989, time 14718.41ms, mfu 30.23%\n",
            "iter 5510: loss 0.9302, time 106.19ms, mfu 30.57%\n",
            "iter 5520: loss 0.8774, time 105.51ms, mfu 30.90%\n",
            "iter 5530: loss 0.8688, time 105.91ms, mfu 31.18%\n",
            "iter 5540: loss 0.9210, time 105.92ms, mfu 31.44%\n",
            "iter 5550: loss 0.8954, time 105.75ms, mfu 31.67%\n",
            "iter 5560: loss 0.8920, time 105.86ms, mfu 31.88%\n",
            "iter 5570: loss 0.9051, time 106.02ms, mfu 32.06%\n",
            "iter 5580: loss 0.8968, time 106.07ms, mfu 32.22%\n",
            "iter 5590: loss 0.8793, time 105.67ms, mfu 32.38%\n",
            "iter 5600: loss 0.8838, time 106.16ms, mfu 32.51%\n",
            "iter 5610: loss 0.9081, time 106.03ms, mfu 32.62%\n",
            "iter 5620: loss 0.8798, time 105.33ms, mfu 32.75%\n",
            "iter 5630: loss 0.8912, time 105.99ms, mfu 32.85%\n",
            "iter 5640: loss 0.9361, time 106.03ms, mfu 32.93%\n",
            "iter 5650: loss 0.8892, time 105.63ms, mfu 33.02%\n",
            "iter 5660: loss 0.8871, time 105.52ms, mfu 33.10%\n",
            "iter 5670: loss 0.8870, time 105.63ms, mfu 33.17%\n",
            "iter 5680: loss 0.9129, time 106.37ms, mfu 33.21%\n",
            "iter 5690: loss 0.9249, time 106.36ms, mfu 33.25%\n",
            "iter 5700: loss 0.8878, time 105.30ms, mfu 33.32%\n",
            "iter 5710: loss 0.8550, time 105.94ms, mfu 33.36%\n",
            "iter 5720: loss 0.8991, time 105.44ms, mfu 33.41%\n",
            "iter 5730: loss 0.8591, time 106.01ms, mfu 33.44%\n",
            "iter 5740: loss 0.8922, time 105.17ms, mfu 33.49%\n",
            "step 5750: train loss 0.7001, val loss 1.3489\n",
            "iter 5750: loss 0.8722, time 14733.69ms, mfu 30.16%\n",
            "iter 5760: loss 0.8597, time 105.17ms, mfu 30.54%\n",
            "iter 5770: loss 0.8611, time 105.22ms, mfu 30.88%\n",
            "iter 5780: loss 0.8790, time 106.43ms, mfu 31.15%\n",
            "iter 5790: loss 0.8836, time 105.73ms, mfu 31.41%\n",
            "iter 5800: loss 0.8640, time 105.62ms, mfu 31.65%\n",
            "iter 5810: loss 0.8653, time 105.66ms, mfu 31.87%\n",
            "iter 5820: loss 0.8852, time 104.98ms, mfu 32.09%\n",
            "iter 5830: loss 0.8497, time 105.60ms, mfu 32.26%\n",
            "iter 5840: loss 0.8926, time 106.26ms, mfu 32.39%\n",
            "iter 5850: loss 0.8985, time 105.45ms, mfu 32.54%\n",
            "iter 5860: loss 0.9003, time 105.50ms, mfu 32.67%\n",
            "iter 5870: loss 0.8200, time 105.30ms, mfu 32.80%\n",
            "iter 5880: loss 0.8494, time 104.90ms, mfu 32.92%\n",
            "iter 5890: loss 0.8936, time 105.35ms, mfu 33.02%\n",
            "iter 5900: loss 0.8830, time 105.74ms, mfu 33.10%\n",
            "iter 5910: loss 0.8738, time 105.64ms, mfu 33.17%\n",
            "iter 5920: loss 0.8754, time 106.37ms, mfu 33.21%\n",
            "iter 5930: loss 0.8705, time 105.74ms, mfu 33.27%\n",
            "iter 5940: loss 0.8608, time 105.88ms, mfu 33.31%\n",
            "iter 5950: loss 0.8144, time 105.78ms, mfu 33.36%\n",
            "iter 5960: loss 0.8818, time 105.82ms, mfu 33.40%\n",
            "iter 5970: loss 0.8483, time 106.22ms, mfu 33.42%\n",
            "iter 5980: loss 0.8643, time 105.32ms, mfu 33.47%\n",
            "iter 5990: loss 0.8855, time 105.46ms, mfu 33.51%\n",
            "step 6000: train loss 0.6797, val loss 1.3559\n",
            "iter 6000: loss 0.8508, time 14707.84ms, mfu 30.18%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample inference:"
      ],
      "metadata": {
        "id": "zs8MFVBI0aOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import pickle\n",
        "# from contextlib import nullcontext\n",
        "# import torch\n",
        "import tiktoken\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "num_samples = 5 # number of samples to draw\n",
        "max_new_tokens = 500 # number of tokens generated in each sample\n",
        "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        "seed = 1337\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
        "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "# model\n",
        "# init from a model saved in a specific directory\n",
        "ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
        "checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "gptconf = GPTConfig(**checkpoint['model_args'])\n",
        "model = GPT(gptconf)\n",
        "state_dict = checkpoint['model']\n",
        "unwanted_prefix = '_orig_mod.'\n",
        "for k,v in list(state_dict.items()):\n",
        "    if k.startswith(unwanted_prefix):\n",
        "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "\n",
        "model.eval()\n",
        "model.to(device)\n",
        "if compile:\n",
        "    model = torch.compile(model) # requires PyTorch 2.0 (optional)\n",
        "\n",
        "meta_path = 'meta.pkl'\n",
        "print(f\"Loading meta from {meta_path}...\")\n",
        "with open(meta_path, 'rb') as f:\n",
        "    meta = pickle.load(f)\n",
        "stoi, itos = meta['stoi'], meta['itos']\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "\n",
        "\n",
        "# prompt\n",
        "start = \"\\n\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
        "start_ids = encode(start)\n",
        "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "\n",
        "# run generation\n",
        "with torch.inference_mode():\n",
        "    with ctx:\n",
        "        for k in range(num_samples):\n",
        "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "            print(decode(y[0].tolist()))\n",
        "            print('---------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvy_z1tx0ZkA",
        "outputId": "59c6079d-2bd4-4164-a5b2-cf223595eb6c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 109.43M\n",
            "Loading meta from meta.pkl...\n",
            "\n",
            "יום שנאמר ושם המפורש הגדול אשר נתן להם ולא נתן להם קלון אלא לפיכך כתיב ואשמיד פריו ממעל ושרשיו מתחת כסאו שרשיו מתחת כסא הכבוד שנאמר ויעמד פרח ויפריו מתחת כנפיו וישב ישראל וגו' ויחנו בחרט למה שנו רבותינו למה שהיתה המילה שמתחת כנפיו לו אותה שעה לא חטא ויאמר ה' אל משה נטה ידך על המטה והרבה רגליך את הסכנה לו כשישראל כותשין את הסכנה ונופלין במטה ומה כתיב שם ויבאו שני המלאכים סדומה ויחנו לפני המלאכים הקב\"ה אמר ר' אלעזר ברבי שמעון בן יוחי אומר מאזני ומה היה עם לבבך היה עמלק אמר ר' יוסי בר חנינא אם יהיה\n",
            "---------------\n",
            "\n",
            "בושה זו רשות לאחרים ויבאו עלינו כי אני ה' אלהיך וגו' אמרו לו הרי עתיד הקב\"ה לעשות את הדין להם את הדין וכיון שהוא אומר עד עכשיו אין אנו יודעין שאין הרי מתים שאין לנו כלום אלא במידה שאמרו נהיה ראשית תחת הנשים וגו' וזה הדבר כתוב אחד אומר ויודע ה' אל משה כתב לך את הדבר אשר דברת לאמר בהעלתך את הדברים האלה כתב לך את הדברים האלה מה כתב אחריו והודעתם לבניך ובניך למשה ומה אמר להם הקב\"ה אתם מבקשים להקריב לפני לא כן אלא קרבנות שנאמר כה אמר ה' בני בכורי ישראל אמר ה' בני לכך כתיב וידבר ה' אל משה במדבר סיני ז\n",
            "---------------\n",
            "\n",
            "שר ויחר אף משה הוא שהוא שומע באחד לדברים אלא לבשר ודם שהוא מחרף לבו כל הדברים שלא היה גוזר ומבקש רחמים לומר שלא יאמרו מי יתננו מיד וישלח יעקב ויקרא לרחל וללאה השדה אשריהם מי מתו בבואם וגו' ד\"א לרחל למי מתו בביאה אמר ר' אבהו איני כן אלא השדה הזו כל הדברים שנאמר ויהיו אוכלין אותם בשדה ומי נודדים והקריע להם שנאמר ויוציאה מחץ מארץ מצרים ישכון חלש כנפים לחלשים וגו' ישכנו בדמים שנאמר מן השמים ונאמר לחולש מצרים ישכנו בדמים נתתי להם שכנו בדמים שנאמר ויעש בצלאל את הארץ וגו' ומן השמים נתים והארץ וגו' וכן \n",
            "---------------\n",
            "\n",
            "ומו ויפץ ה' אותם אל פרעה וגו' מה כתיב שם וישב משה את ידיו מזקני ישראל ילמדנו רבינו מהו שישב מלמד שהלך למדינה רואה ואומר אותו האיש היה מתבייש נפשו של משה אהרן היה מתבייש מהו שיהא נפשו מתפלל משה רבינו כמה גדול כל המזבח נקראו באותה שעה אמר לו הקדוש ברוך הוא אהרן ומרים למשה אחר אמר משה ריבונו של עולם איני מבקש שלא בראתי עולמי אלא עליך שאתה הולך ואתה אומר לי על כל דבר ודבר לך אמור הלא דבר הוא דבר הוא אמר לו דבר הוא עד ה' עד ה' אמר לו אע\"פ שתפסיד שתשחוט את הכל ואבדתם אין אתה מוצא בשעה שבאו פלשתים באות\n",
            "---------------\n",
            "\n",
            "דחד קטן לה קטן ועקרבים מה כתיב למעלה מן הענין ויסעו מרעמסס סכותה דבר אל בני ישראל ויסעו מה כתיב למעלה מן הענין ויסעו בני ישראל מרעמסס סכותה דבר אחר ויסעו בני ישראל את עמלק מה כתיב למעלה מן הענין ואת עמלק מה כתיב למעלה מן הענין והיתה עולה לפני ה' אמר לו הקדוש ברוך הוא רב לך בשליחותי את ישראל כדי שיסעו עליהם בי המשתה וכתיב ויקח אהרן את עצמות יוסף וכתיב ויקח את עצמות יוסף בצלאל את חצי הלילה לא היה לו לא היה לו לבית קדשי הקדשים ולא היה להם לבית קדשים אלא שהיו לו עבדים הרבה והיה מסיח עמו עד עכשיו והי\n",
            "---------------\n"
          ]
        }
      ]
    }
  ]
}